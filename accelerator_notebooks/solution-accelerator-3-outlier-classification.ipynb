{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "263322f9-3172-436a-b104-62c71e9d5b58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Introduction\n",
    "Understanding real time anomalies  in water level conditions is critical for decision makers in fields such as disaster preparedness and recovery, emergency responders, public safety, etc. Flagging 'abnormal' or outlier conditions in real time can significantly save lives and reduce property damage in high water scenarios. While outlier detection should be tailored to the specific use case, this accelerator notebooks aims to illustrate a few ways that customers can layer outlier detection methods, and alert notifications, on top of the data available in the Divirod data lake. \n",
    "\n",
    "Additionally, users may want to send automated alerts when the water level crosses a threshold boundary (say from 'normal' to 'high' or 'high' to 'extremely high'). Divirod advises implementing some version of an outlier detection method to filter out sending alerts that may be a result of faulty measurement readings or manual input errors on the side of the provider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "137da5d3-125c-4f14-93b7-04fe457e60a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Goal\n",
    "The goal of this notebook is to demonstrate how users can implement outlier detection workflows on top of water level data in order to notify decision makers of the occurrence of abnormal water levels in real time. The examples in this notebook pull from Divirod's Water Level Index (WLI) - an in-house water risk scoring system. The WLI score is calculated each day and indicates where the latest water level measurement falls in relation to historical measurements from that gauge. This index can be used as an indicator for when current water levels are either extremely high or extremely low when compared to historical trends. The WLI interactive map and methodology document can be found [here](https://wli.divirod.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4097010d-b3a7-4c1d-9c6f-be75b03d62c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The below graphic serves as a reference to where the content within this notebook sits in relation to Divirod's overall data ingestion pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5db1ab-32fb-45e1-9f71-bc2e9c4ded04",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "image_path = './../images/architecture_diagram_outlier_classification.png'\n",
    "\n",
    "# Function to get Base64 string of an image\n",
    "def get_image_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "image_base64 = get_image_base64(image_path)\n",
    "\n",
    "# Create an HTML string with the Base64 embedded and width set to 100%\n",
    "html = f'<img src=\"data:image/jpg;base64,{image_base64}\" style=\"width:100%;\">'\n",
    "\n",
    "# Display the image in the notebook\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdd67105-811f-440e-8788-e01b98242d6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Example Workflow Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f9f2953-c648-4761-9cf6-bfafa20e9eed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Outlier Detection Case 1\n",
    "One potential outlier detection system may be classification. Building off the workflow outlined in the Solutions Accelerator Part 2 - Detection of Normal values, users may want to classify each daily average height and understand where it falls in relation to predefined threshold values. In our example, this would allow the user to know if daily water level conditions are 'extremely high', 'high', 'normal', 'low', or 'extremely low'. Understanding these daily conditions may be extremely useful for decision makers in operations... How full, or low, is my reservoir currently? Am I at risk of not having enough water available for daily cooling operations, etc. While the thresholds in this example are pre-defined, users can of course tailor their own threshold values to fit their needs.\n",
    "\n",
    "Using this methodology, alert notifications could be sent out when these thresholds are crossed. For example, in the figure below, and alert notification could be sent the first time the water level crosses from 'high' to 'extremely' high to give decision makers notice that water levels are much higher than expected for that period of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0211d2c2-cf4f-4b8f-b07b-40466e56cc2d",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "image_path = './../images/river_classification_ex.png'\n",
    "\n",
    "# Function to get Base64 string of an image\n",
    "def get_image_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "image_base64 = get_image_base64(image_path)\n",
    "\n",
    "\n",
    "html = f'<img src=\"data:image/jpg;base64,{image_base64}\" style=\"width:60%; height: auto; display: block; margin: auto;\">'\n",
    "\n",
    "\n",
    "# Display the image in the notebook\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b906e600-fc93-4cca-9edc-5734b5a5a3fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Outlier Detection Case 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72bc9025-aefa-451d-b9c3-0a2d00530947",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Code Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9943ed0-aa03-47c9-9675-eeb4e6d4247a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63847d28-9083-4b03-bd97-b8d2dcd56496",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, TimestampType, IntegerType\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import dataframe as sparkDataFrame\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "314a39cf-8323-46fe-9242-2f12ead46633",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read in Supporting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc8d2c3-40dc-4270-bd44-80f6e7c9d838",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "threshold_df = pd.read_csv('./../data/billings_thresholds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15608e53-398d-453d-bf97-12e2bb164583",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "threshold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592b7796-eb26-4339-ae96-d3c5f9e4d5cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# again, in reality, these would be referenced in a larger delta table in the delta lake with these values saved for each instrument id and re-calculated on a defined schedule as new data comes in\n",
    "extreme_high_threshold = threshold_df['extreme_high_threshold'][0]\n",
    "high_threshold = threshold_df['high_threshold'][0]\n",
    "low_threshold = threshold_df['low_threshold'][0]\n",
    "extreme_low_threshold = threshold_df['extreme_low_threshold'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a270c12-6fca-4475-a110-9b89d4d7ba72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "billings_2022_quality_checked_path = \"./../data/billings_11074_all_measurements_2022_with_dq\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a007ec-fc4c-4803-b406-9ce7b876cb4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Stream Configuration\n",
    "This simulates our data stream coming out of our ETL pipeline - these same columns are present for all instruments in the data lake (both Divirod instruments, as well as third party)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a9f6d23-17be-4f15-8937-5549a7c308ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define schema\n",
    "billings_data_schema = StructType([\n",
    "    StructField(\"instrument_id\", IntegerType(), True),\n",
    "    StructField(\"epoch\", LongType(), True),\n",
    "    StructField(\"time\", TimestampType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"height_native\", DoubleType(), True),\n",
    "    StructField(\"precision\", DoubleType(), True),\n",
    "    StructField(\"datum_native\", StringType(), True),\n",
    "    StructField(\"accuracy_native\", DoubleType(), True),\n",
    "    StructField(\"height_unified\", DoubleType(), True),\n",
    "    StructField(\"datum_unified\", StringType(), True),\n",
    "    StructField(\"accuracy_unified\", DoubleType(), True),\n",
    "    StructField(\"provider_id\", IntegerType(), True),\n",
    "    StructField(\"data_level\", StringType(), True),\n",
    "    StructField(\"confidence\", DoubleType(), True),\n",
    "    StructField(\"last_update\", TimestampType(), True),\n",
    "    StructField(\"last_update_epoch\", LongType(), True),\n",
    "    StructField(\"qualitative_metric\", StringType(), True),\n",
    "    StructField(\"diagnostic_metric\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DivirodSolutionAccelerator3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4801ce6f-932e-44de-9710-194b8c2c16d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read Parquet file as streaming DataFrame\n",
    "simulated_data_stream = spark.readStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .schema(billings_data_schema)\\\n",
    "    .load(billings_2022_quality_checked_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddb0171d-196d-4b89-8ebd-f8420a231743",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For this workflow, we are going to simulate daily average height values. The WLI uses daily average values for each gauge location to calculate the daily WLI index. More details on this process can be found in the methodology on the [WLI Map Page](https://wli.divirod.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d929b71-970a-470c-a26d-5cc891cad85d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group stream into daily averages (really, this would be done in SQL and not in the stream itself, this is just to simulate the automation in place for this process)\n",
    "# Sort by time series order\n",
    "daily_data_agg = simulated_data_stream \\\n",
    "    .groupBy(F.window(F.col(\"time\"), \"1 day\")) \\\n",
    "    .agg(F.avg(\"height_native\").alias(\"daily_height\")) \\\n",
    "    .withColumn(\"date\", F.col(\"window.start\")) \\\n",
    "    .drop(\"window\") \\\n",
    "    .orderBy(\"date\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34682ef3-4198-4022-b734-603d181cea3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Outlier Detection\n",
    "There are many ways users can implement outlier detection methods. Below are two potential workflows."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae85151-59c1-42d1-89f2-36486651426c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Outlier Detection Case 1 - Daily Evaluation of Water Risk Category\n",
    "Users can classify each daily average height and understand where it falls in relation to the predefined threshold values. This would allow the user to know if water level conditions are 'extremely high', 'high', 'normal', 'low', or 'extremely low'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05787daf-ae86-422b-82a4-ed661f054beb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_flag_outlier_categories(df:sparkDataFrame) -> sparkDataFrame:\n",
    "  \"\"\"\n",
    "  Assigns a water level status to each entry in the DataFrame based on predefined threshold values for daily water heights.\n",
    "\n",
    "  This function evaluates the 'daily_height' column in the input DataFrame and classifies each record into one of five categories: 'extreme_high', 'high', 'normal', 'low', or 'extreme_low'. The classification is based on comparison against the predefined thresholds.\n",
    "\n",
    "  Parameters:\n",
    "      df (sparkDataFrame): A Spark DataFrame containing at least a 'daily_height' column with numeric values representing daily water levels.\n",
    "\n",
    "  Returns:\n",
    "      sparkDataFrame: A DataFrame identical to the input but with an additional column 'water_level_status' indicating the water level category for each record.\n",
    "  \"\"\"\n",
    "  df = df.withColumn(\n",
    "    \"water_level_status\",\n",
    "    F.when(F.col(\"daily_height\") >= extreme_high_threshold, \"extreme_high\")\n",
    "        .when(((F.col(\"daily_height\") < extreme_high_threshold) & (F.col(\"daily_height\") >= high_threshold)), \"high\")\n",
    "        .when(F.col(\"daily_height\") <= extreme_low_threshold, \"extreme_low\")\n",
    "        .when(((F.col(\"daily_height\") > extreme_low_threshold) & (F.col(\"daily_height\") <= low_threshold)), \"low\")\n",
    "        .otherwise(\"normal\")\n",
    ")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d5cea1-e4c7-413b-953a-0fc1cc2b6171",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply function to streamed data set with transform\n",
    "flag_outliers_case_1_df = daily_data_agg.transform(calculate_flag_outlier_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_outliers_example1_query = (\n",
    "    flag_outliers_case_1_df\n",
    "    .withWatermark(\"date\", \"10 minutes\") \\\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")  # Use \"complete\" output mode\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a7a84a-6b97-4373-96fb-75cb5f496f66",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Outlier Detection Case 2 - Daily Evaluation of Custom Water Risk Thresholds\n",
    "Another potential option for outlier detection could be to create user defined thresholds on the calculated Water Level Index (WLI)  values. Divirod's assigned WLI values range from -100 (extremely  low water level) to 100 (extremely high water level). Rather than using Divirod's built in water level classification system, users may want to define their own outlier detection method based on these WLI values.\n",
    "\n",
    "This example assumes that the user wants to send an alert to decision makers when the defined threshold is crossed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566cdd8c-8f13-46fd-a5ab-73c5b7761d77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_wli(df: sparkDataFrame) -> sparkDataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the WLI value for each daily height reading in the input dataset based on pre-defined extreme condition threshold values.\n",
    "\n",
    "    Parameters:\n",
    "        df (sparkDataFrame): A Spark DataFrame containing at least a 'daily_height' column with numeric values representing daily water levels.\n",
    "\n",
    "    Returns:\n",
    "        sparkDataFrame: A DataFrame identical to the input but with an additional column 'wli' indicating the water level index value for each record.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\n",
    "        \"wli\", (((F.col('daily_height')-extreme_low_threshold)/(extreme_high_threshold-extreme_low_threshold))*2 -1.0)*100\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d132862-433a-4625-b2fa-1034c023f12e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define variables for WLI outliers (these should be defined based on customer needs)\n",
    "HIGH_WLI_THRESHOLD = 90\n",
    "LOW_WLI_THRESHOLD = -90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3150895-0ff0-4dd9-94a6-47cdd0800673",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def flag_outliers_based_on_user_thresholds(df: sparkDataFrame) -> sparkDataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates a 'wli' (water level index) column in a Spark DataFrame and flags each record based on user-defined thresholds for high and low water levels.\n",
    "\n",
    "    This function checks the 'wli' value for each entry in the DataFrame and assigns an alert status in a new column 'alert'. Entries are flagged as 'Alert - High' if they meet or exceed a predefined high threshold (HIGH_WLI_THRESHOLD) or fall below a predefined low threshold (LOW_WLI_THRESHOLD). All other entries are marked as 'Normal'.\n",
    "\n",
    "    Parameters:\n",
    "        df (sparkDataFrame): A Spark DataFrame containing at least a 'wli' column with numeric values.\n",
    "\n",
    "    Returns:\n",
    "        sparkDataFrame: A DataFrame similar to the input but with an additional 'alert' column showing the alert status for each record.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.withColumn(\n",
    "    \"alert\",\n",
    "        F.when(F.col(\"wli\") >= HIGH_WLI_THRESHOLD, \"Alert - High\")\n",
    "            .when(F.col(\"wli\") <= LOW_WLI_THRESHOLD, \"Alert - High\")\n",
    "            .otherwise(\"Normal\")\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144a9ac5-b72b-4936-8d3a-7dc0e9bf0e3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply wli calculation to streamed data set with transform \n",
    "daily_agg_wli = daily_data_agg.transform(calculate_wli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8bcdc7-faad-48ba-bc5f-af7f88943eb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply outlier detection to streamed data set with transform \n",
    "flag_outliers_case_2_df = daily_agg_wli.transform(flag_outliers_based_on_user_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_outliers_example2_query = (\n",
    "    flag_outliers_case_2_df\n",
    "    .withWatermark(\"date\", \"10 minutes\") \\\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"complete\")  # Use \"complete\" output mode\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82f0198a-b3e0-4629-a7a1-49591ab08560",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1956410626873164,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Solution Accelerator Part 3 - Classification for Outliers",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
