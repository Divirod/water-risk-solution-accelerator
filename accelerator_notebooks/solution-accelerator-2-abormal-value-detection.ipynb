{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c90d74df-611c-41f8-a054-9d51226013bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Introduction\n",
    "Having access to quality checked data at the push of a button is just one piece of the puzzle. That capability alone doesn't necessarily translate to impactful water risk assessment, but it does provide the foundation for it. Divirod uses the millions of records available in the data lake to establish a \"normal\" range of water level for each location derived from historical measurements. This process of defining 'normal' ranges enables the identification of anomalies, such as sudden spikes or dips in water levels, abnormal flow rates, or unexpected weather patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d88723f1-92ce-4195-ba62-01f4eaca4975",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Goal\n",
    "The goal of this notebook is to demonstrate how the data in Divirod's data lake can be used to calculate thresholds for abnormal water level conditions. The workflow outlined in this notebook is currently implemented by Divirod, and is just one of many possible workflows that customers can build using Divirod's data platform. Customers can either access this dataset directly from Divirod or define customized workflows in a similar manner more tailored to their specifically defined risk thresholds.\n",
    "\n",
    "This notebook will contain a combination of detailed code for execution of this process, as well as explanatory text intermingled throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45667582-2072-4b50-90ff-d68e2cf8b1c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The below graphic serves as a reference to where the content within this notebook sits in relation to Divirod's overall data ingestion pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89b723ca-743e-4483-9f41-abfcde525dce",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "image_path = './../images/architecture_diagram_detect_normal_vals.png'\n",
    "\n",
    "# Function to get Base64 string of an image\n",
    "def get_image_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "image_base64 = get_image_base64(image_path)\n",
    "\n",
    "# Create an HTML string with the Base64 embedded and width set to 100%\n",
    "html = f'<img src=\"data:image/jpg;base64,{image_base64}\" style=\"width:100%;\">'\n",
    "\n",
    "# Display the image in the notebook\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9a27f19-8c07-425f-9bc5-d119df9f3d9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Example Workflow Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "822e18fe-c7ea-4764-8824-8f2c084cbb29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Calculate Water Risk Thresholds\n",
    "This process is meant to showcase how users can apply their own workflows on top of the Divirod data lake. This example is taken from the workflow used to define the 'normal' and 'extreme' conditions used as the foundation for [Divirod's Water Level Index (WLI)](https://wli.divirod.com/).\n",
    "\n",
    "A few caveats:\n",
    "* In reality, the defined thresholds based on *all* the data in the delta lake for a particular instrument. For the purposes of this demo however, thresholds will be calculated using only the 2022 data from billings in the provided parquet file.\n",
    "* This is a static process - this calculation is not performed on streamed data, but on a static pull of ALL the data available for each instrument.\n",
    "* The example threshold values are calculated for a riverine system, which displays different seasonal trends than coastal or reservoir systems. This example applies *only* to riverine systems. More details on how Divirod calculates water risk thresholds based on system dynamics can be found in the [WLI Methodology Documentation](https://wli.divirod.com/methodology.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63457223-6b76-4dbb-8553-bf62d5915528",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Water Level Status Classification Equations\n",
    "\n",
    "Divirod has chosen to assign one of five water level status codes to each gauge in the data lake: ‘extremely high,’ ‘high,’ ‘normal,’ ‘low,’ and ‘extremely low.’ For all water body systems, these classifications are assigned based on the statistical distribution of all the data records for each gauge.\n",
    "\n",
    "The equations for these classifications are given by:\n",
    "\n",
    "$$ WL_e = \\mu_{wl} \\pm X_e \\cdot \\sigma_{wl} $$\n",
    "$$ WL_{hl} = \\mu_{wl} \\pm X_{hl} \\cdot \\sigma_{wl} $$\n",
    "\n",
    "\n",
    "Where:\n",
    "* *WL<sub>e</sub>* = Extreme water level\n",
    "* *X<sub>e</sub>* = Extreme multiplier $$\n",
    "* *WL<sub>hl</sub>*= High/low water level \n",
    "* *X_<sub>hl</sub>* = High/low multiplier \n",
    "* *μ_<sub>wl</sub>* = Mean water level\n",
    "* *σ_<sub>wl</sub>* = Standard deviation of recorded water levels \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d4ee203-bcc9-4762-bf19-cb832427ce51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Classification for River Systems\n",
    "River systems display a unique seasonal water level pattern. Heavily influenced by winter run off and\n",
    "spring rainstorms, both river stage (water level) and flow rate tend to increase very rapidly in the\n",
    "months coming out of winter, slowly tapering off later into the summer. Because of the unique system\n",
    "characteristics of rivers and their relationship to spring runoff, the annual water levels for a specific river\n",
    "location generally do not follow a normal distribution. An analysis of all records associated with river\n",
    "systems in our data lake indicated that the data is heavily right skewed. To account for this, the water\n",
    "level readings for river systems were transformed by taking their natural log before applying the\n",
    "statistical method described above. While not always a perfect fit, this transformation brought the\n",
    "series much closer to a normal distribution. After the extreme and high/low thresholds are calculated in\n",
    "the log space, they are transformed back into the original units before each record is assigned a status. The below two charts demonstrate the data from our selected example location before and after the log transform is applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a351ea-1661-4daa-8998-a78fb2cb0b40",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "image_path = './../images/river_right_skewed_data_ex.png'\n",
    "\n",
    "# Function to get Base64 string of an image\n",
    "def get_image_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "image_base64 = get_image_base64(image_path)\n",
    "\n",
    "\n",
    "html = f'<img src=\"data:image/jpg;base64,{image_base64}\" style=\"width:60%; height: auto; display: block; margin: auto;\">'\n",
    "\n",
    "\n",
    "# Display the image in the notebook\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c367607a-a73f-480e-a881-ed42d3a9c01c",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "image_path = './../images/river_normalized_data_ex.png'\n",
    "\n",
    "# Function to get Base64 string of an image\n",
    "def get_image_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "image_base64 = get_image_base64(image_path)\n",
    "\n",
    "\n",
    "html = f'<img src=\"data:image/jpg;base64,{image_base64}\" style=\"width:60%; height: auto; display: block; margin: auto;\">'\n",
    "\n",
    "\n",
    "# Display the image in the notebook\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0903c6e5-8ce6-4829-a223-39c136fc1ac0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The selected extreme multiplier for river systems is a value of 3, meaning that anything greater than 3\n",
    "standard deviations away from the mean in the log space is classified as an “extreme condition”. The\n",
    "high/low multiplier for river systems is a value of 2, meaning any value within 2 standard deviations of\n",
    "the mean are classified as a “normal condition”. The below Figure illustrates this classification for a river\n",
    "gauge on the Yellowstone River at Billings Montana (USGS: 06214500)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f6380a5-0c4d-402e-a57b-52960b318def",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "image_path = './../images/river_classification_ex.png'\n",
    "\n",
    "# Function to get Base64 string of an image\n",
    "def get_image_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "image_base64 = get_image_base64(image_path)\n",
    "\n",
    "\n",
    "html = f'<img src=\"data:image/jpg;base64,{image_base64}\" style=\"width:60%; height: auto; display: block; margin: auto;\">'\n",
    "\n",
    "\n",
    "# Display the image in the notebook\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3363d16-2cfd-4af3-9f34-eee17602ac75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The below code section demonstrates the implementation of this workflow. Customers with access to the Divirod data lake can perform similar threshold calculations or utilized Divirod's pre-defined thresholds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94f7d23c-d4af-4290-9a3d-12f3fdf2b45f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Code Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73b6c2e-00e4-4461-b1ea-787ba96c2bc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cff3999-e6cf-4ce7-b3c4-69374d631b0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448c67be-a0bd-4461-a887-3b45665c0051",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "789d61cf-8417-446e-bb94-b9425c830893",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a36a1ea-6db2-41e9-a7e1-d5efbe9369e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this file path will be updated to the appropriate path in the github repo\n",
    "quality_checked_fp = \"./../data/billings_11074_all_measurements_2022_with_dq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DivirodSolutionAccelerator2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "603f40b3-b3b6-4009-abcf-feff85f1241c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "billings_data_all = spark.read.parquet(quality_checked_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f902483d-f18e-4498-a7d0-cdd2258a01eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# see 'Example Workflow Explanation' section for the reasoning behind the choice of these multipliers\n",
    "river_extreme_multiplier = 3\n",
    "river_high_low_multiplier = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b963934-0380-4be8-bf75-097bfd4b6cc4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Calculate Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02f180cd-38c6-4186-be4d-8a73cea04069",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# take the log of the height to transform to a normal distribution\n",
    "billings_data_all = billings_data_all.withColumn(\"log_height_native\", F.log(F.col(\"height_native\")))\n",
    "\n",
    "# normally, this calculation is applied for all instruments in the data (applied across the entire data lake)\n",
    "# for this example, we will only calculate these values for the Billings location\n",
    "mean_log_height = billings_data_all.agg(F.mean(\"log_height_native\").alias(\"mean_log_height_native\")).collect()[0][\"mean_log_height_native\"]\n",
    "std_log_height = billings_data_all.agg(F.stddev(\"log_height_native\").alias(\"stddev_log_height_native\")).collect()[0][\"stddev_log_height_native\"]\n",
    "\n",
    "# compute thresholds for each instrument in the log space\n",
    "extreme_high_threshold = round(mean_log_height + river_extreme_multiplier * std_log_height, 3)\n",
    "high_threshold = round(mean_log_height + river_high_low_multiplier * std_log_height, 3)\n",
    "low_threshold = round(mean_log_height - river_extreme_multiplier * std_log_height, 3)\n",
    "extreme_low_threshold = round(mean_log_height - river_high_low_multiplier * std_log_height, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617a50ab-5d67-4a43-b9c2-f9c807e7ace5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Save Thresholds\n",
    "In reality, these thresholds will be stored in a delta lake table, joined with the underlying data set, or a similar workflow to tie these thresholds to each instrument in the delta lake to be referenced later in the water risk assessment workflow (see Solution Accelerator Part 3). For this demo, a simple CSV export will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40fae04-ef9f-43d4-a2e8-3d17ea740b61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "threshold_data = {\n",
    "    \"instrument_id\": [11074], # instrument id for our example dataset\n",
    "    \"extreme_high_threshold\": [extreme_high_threshold],\n",
    "    \"high_threshold\": [high_threshold],\n",
    "    \"low_threshold\": [low_threshold],\n",
    "    \"extreme_low_threshold\": [extreme_low_threshold]\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame for expport\n",
    "threshold_df = pd.DataFrame(threshold_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a3a1a4-52e4-43a3-baa8-c059722112e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this has already been exported as a CSV file that can be pulled into the last notebook in this accelerator\n",
    "threshold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Solution Accelerator Part 2 - Detection of Normal Values",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
